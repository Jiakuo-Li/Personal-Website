---
title: "Session 2: Homework 1"
author: "Study Group 4"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(ggrepel)
```

# Introduction

The aim of the specific document is to analyse various datasets using R, and more specifically using the tidyverse and ggplot2 packages. The document is split in 6 chapters, namely four tasks and two challenges. The first task examines alcohol consumption in the different countries of the world, the second analyses a data set of IMDB movie ratings, the third task analyses financial stock data from the New York Stock Exchange (nyse), whilst the 4th analyses a data set from the human resources at IBM. In the first challenge a graph depicting the relationship between white homicide rate and white suicide rate was replicated ready for publication and finally in the second challenge the aim was to reproduce a plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.

# Task 1:  Where Do People Drink The Most Beer, Wine And Spirits?

The aim of the first task is to study the data taken from [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) about the alcohol consumption in different countries. 


```{r, load_alcohol_data}
library(fivethirtyeight)
data(drinks)

```

Let's first have a quick look at the variables present in this data:

```{r glimpse_skim_data}

glimpse(drinks)

```


To begin with we can plot the top 25 countries in consumption of beer, wine and spirits respectively as shown below:

```{r beer_plot}

beer_drinks <- drinks %>% 
  arrange(desc(beer_servings))
top_25 <- head(beer_drinks,25)
ggplot(top_25,aes(x=beer_servings, y=reorder(country,beer_servings)))+
  geom_col(fill="#001e62",alpha=0.7)+
  labs(title="Top 25 Beer Consuming Countries", x="Beer Servings(Servings per person)", y=NULL)+
  theme_bw()+

  NULL

```


```{r wine_plot}

wine_drinks <- drinks %>% 
  arrange(desc(wine_servings))
top_25 <- head(wine_drinks,25)
ggplot(top_25,aes(x=wine_servings, y=reorder(country,wine_servings)))+
  geom_col(fill="#001e62",alpha=0.7)+
  labs(title="Top 25 Wine Consuming Countries", x="Wine Servings(Servings per person)", y=NULL)+
  theme_bw()+
  
  NULL

```


```{r spirit_plot}

spirit_drinks <- drinks %>% 
  arrange(desc(spirit_servings))
top_25 <- head(spirit_drinks,25)
ggplot(top_25,aes(x=spirit_servings, y=reorder(country,spirit_servings)))+
  geom_col(fill="#001e62",alpha=0.7)+
  labs(title="Top 25 Spirit Consuming Countries", x="Spirit Servings(Servings per person)", y=NULL)+
  theme_bw()+
  NULL
```

When looking at the alcohol data, a few different things stood out. Firstly, it appears that there is not a huge amount of overlap for the top 25 countries for beer, wine and spirits, meaning that if a country was in the top 25 for one, it is unlikely that they would be in the top 25 of another. This is a bit surprising because one would think that countries that consume a large amount of one type of alcohol would also consume large amounts of other types. However, this was not the case. It could be argued that countries in the top 25 for one type may not be in the top 25 of others since they have better access and affordability to certain types. For example, Czech Republic is one of the top countries for beer consumption, but is not in the top 25 of wine or spirits. Czech Republic is known for its beer consumption, and that “beer is cheaper than water”, so it is unlikely that citizens there would drink large amounts of anything else when beer is so cheap and available. Additionally, there was not a single country that was in the top 25 for all 3 types of alcohol. Six countries (Belgium, Germany, Ireland, Romania, Slovenia, and the Netherlands) were in the top 25 for beer and wine. Poland, Lithuania, and Latvia were in the top 25 for beer and spirits. All countries that were in two different top 25 groups were European, which could mean that European countries consume more alcohol, and more of a diverse selection, than other continents.
Subsidised products for some countries, hence appearing in the top 25.


# Task 2: Analysis of movies- IMDB dataset

In this task We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset). As always lets first have a glimpse at our dataset.

```{r,load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)
options(scipen=999)
```

As we can see, besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the dataset has a number of different variables as follows:

- `gross` : The gross earnings in the US box office, not adjusted for inflation
- `budget`: The movie's budget 
- `cast_facebook_likes`: the number of facebook likes cast members received
- `votes`: the number of people who voted for (or rated) the movie in IMDB 
- `reviews`: the number of reviews for that movie
- `rating`: IMDB average rating 

Initially, we need to clean the dataset and prepare it for subsequent analysis. Using the below code we will inspect whether our data has any missing or duplicated values.

```{r}
sapply(movies,function(x) sum(is.na(x)))
anyDuplicated(movies)
```

As we cans see on the table above, the data did not have any missing or duplicated values, so we can proceed with our dataset as is. Firstly, it would be interesting to examine the dataset with respect to the number of movies per genre.

```{r}
movies_genre <- count(movies,genre, sort = TRUE)
names(movies_genre)<-c("Movie Genre","Number of Movies") 
movies_genre
```

Moreover,  we can create a table where we can observe the average gross earning and average gross budget of each genre of movies and hence calculate the average % return on budget for each type of movies.

```{r}
gross_budget <- movies %>%
  group_by(genre) %>%
  summarise(avg_gross = mean(gross)/1000000,
            avg_budget = mean(budget)/1000000) %>%
  mutate(return_on_budget = (avg_gross/avg_budget) * 100) %>%
  arrange(desc(return_on_budget))
gross_budget$avg_gross <- round(gross_budget$avg_gross)
gross_budget$avg_budget <- round(gross_budget$avg_budget)
gross_budget$return_on_budget <- round(gross_budget$return_on_budget)
names(gross_budget)<-c("Movie Genre","Avg Gross Earning(M$)","Avg Gross Budget(M$)","Return on Budget(%)")
gross_budget

```

Another interesting insight to draw from the data would be to investigate the corelation between sales and the director of each film. 

```{r}
gross_director <- movies %>%
  group_by(director) %>%
  summarize(total_gross = round(sum(gross)/1000000),
            mean_gross = round(mean(gross)/1000000),
            med_gross = round(median(gross)/1000000),
            sd_gross = sd(gross)) %>%
  arrange(desc(total_gross))
gross_director$total_gross <- round(gross_director$total_gross)
gross_director$mean_gross <- round(gross_director$mean_gross)
gross_director$med_gross <- round(gross_director$med_gross)
names(gross_director)<-c("Director","Total Gross(M$)","Average Gross(M$)","Median Gross(M$)","SD")
head(gross_director,15)
```

As expected, Steven Spielberg is at the top of the list with the highest average gross sales per movie. It is important to note however that average gross sales do not necessarily mean profitability, as we have no information regarding the average cost of the movies discussed.

Finally, we will create a table that  describes how ratings are distributed by genre and visually represent the distribution through a box plot.

```{r}
genre_rating <- movies %>%
  group_by(genre) %>%
  summarize(avg_rating = round(mean(rating),1),
            min_rating = min(rating),
            max_rating = max(rating),
            med_rating = round(median(rating),1),
            sd_rating = sd(rating))

names(genre_rating)<-c("Genre","Average Rating","Min","Max","Median","SD")
genre_rating

ggplot(movies, aes(x = genre, y = rating, color = genre)) +
  geom_boxplot() +
  labs(title = "Movie Ratings by Genre", x = "Movies Genre", y = "Ratings" ) +
  theme( axis.text.x=element_text(angle = 45))+
  NULL
```

On the scatterplot below we examine the relationship between movie revenues and Facebook likes.To achieve that we have plotted movie revenues on the y-axis and Facebook likes on the x-axis as shown below. Nevertheless, a clear correlation pattern cannot be established. 

```{r, gross_on_fblikes}
ggplot(movies, aes(x = cast_facebook_likes, y = gross)) +
  geom_point() +
  labs(title = "Relationship Between Movies Revenue and Facebook Likes", x = "Facebook Likes", y = "Movie Revenue")+
  theme(axis.text = element_blank())+
  theme(axis.ticks = element_blank()) + 
  NULL
```

 Along the same lines, we can examine also the relationship between the revenues generated and budget required for each movie. In fact as it can be seen in the figure below, there is a positive relationship between the movie revenue and budget so the budget is likely to be a good predictor of movie revenue.

```{r, gross_on_budget}
ggplot(movies, aes(x = budget, y = gross)) +
  geom_point() +
  labs(title = "Relationship Between Movies Budget and Revenue", x = "Movies Budget", y = "Movie Revenue")+
  theme(axis.text = element_blank())+
  theme(axis.ticks = element_blank())
```
  
Finally, in the below figure we inspect the relationship between movie rating and revenue for each type of movies and two points are worth discussing:   

1. In some genre(like Triller), the data size is so small that it is hard for us to infer the relationship between gross revenue and rating.
2. For crime, the ratings are basically in the mid-high range while the revenues stay in a relatively low range.

```{r, gross_on_rating}
ggplot(movies, aes(x = rating, y = gross)) +
  geom_point() +
  labs(title = "Relationship Between Movies Ratings and Revenue", x = "Movies Ratings", y = "Movie Revenue") +
  facet_wrap(~ genre)+
  theme(axis.text = element_blank())+
  theme(axis.ticks = element_blank()) + 
  NULL
```

# Task 3: Returns of financial stocks

In this task of the report we use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns. 

To begin with, we create a dataset containing 508 stocks listed on the NYSE. The dataset has a number of different variables as follows:

- `symbol` : the Ticker symbol of each stock
- `name`: the name of the listed company 
- `ipo_year`: the year the company completed its Initial Public Offering and got listed
- `sector`: the sector the company operates in 
- `industry`: the industry the company operates in
- `summary_quote`: the online link to the company's quote price


```{r load_nyse_data, message=FALSE, warning=FALSE}

# Download stock data using the tidyquant package
nyse <- read_csv(here::here("data","nyse.csv"))

```

Based on this dataset, we can create a table and a bar plot that show the number of companies per sector.

```{r companies_per_sector}

# Create a table with the number of firms in each sector
nyse_by_sector <- nyse %>% 
  group_by(sector) %>% 
  summarise(n = n())

# Plot a bar chart with the 
ggplot(nyse_by_sector, aes(x = reorder(sector, -n), y = n)) +
  geom_col(fill="#001e62",alpha=0.7)+
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(title = "Number of NYSE companies per sector", x = "Sector", y = "Number of listed companies")+
  NULL

```

Next, let's choose some nyse stocks and their ticker symbols and download some data. We have selected to examine the stocks from Alibaba (BABA), Visa (V), Procter & Gamble (PG), Coca-cola (KO), Boeing (BA), Nike (NKE), AstraZeneca (AZN) and the SP500 ETF (Exchange Traded Fund) for a period approximately 10 years. The dataset has a number of different variables as follows:

- `symbol` : the Ticker symbol of each stock
- `date`: stock price date 
- `open`: the price of the stock at opening of the stock exchange
- `high`: the max price of each stock on each day 
- `low`: the min price of each stock on each day
- `close`: the price of the stock at closing of the stock exchange
- `volume`: the number of stocks traded that day
- `adjusted`: the adjusted closing price of the stock, which adjusts for any stock splits or dividends paid

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}

myStocks <- c("BABA","V","PG","KO","BA","NKE","AZN","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2020-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

To proceed we will calculate the average % returns of each stock. Given the adjusted closing prices, our first step is to calculate daily, monthly and annual returns.


```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

Moreover, we can summarise monthly returns for each of the stocks, calculating the minimum, maximum, median, mean and standard deviation of the monthly returns for each stock considered, as well as plotting the distribution for each of the stocks.

```{r summarise_monthly_returns}

summary_monthly_returns <- myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  summarise(min = min(monthly_returns), max = max(monthly_returns), median = median(monthly_returns), mean = mean(monthly_returns), sd = sd(monthly_returns)) %>% 
  mutate(min = sprintf("%0.4f", min), max = sprintf("%0.4f", max), median = sprintf("%0.4f", median), mean = sprintf("%0.4f", mean), sd = sprintf("%0.4f", sd))

summary_monthly_returns

```


```{r density_monthly_returns, cache=TRUE}

ggplot(myStocks_returns_monthly, aes(x = monthly_returns)) + 
  geom_density(fill = "grey", color = "black", alpha = 0.4) + 
  facet_grid(symbol ~ .) + 
  labs(title = "Selected stocks monthly returns distribution", x = "Monthly returns (%)" , y = "Frequency of returns", color="Stock Tickers")+
  NULL

```


The graph above illustrates the distribution of monthly returns of seven company stocks, namely Astrazeneca (AZN), Boeing (BA), Alibaba (BABA), Coca-cola (KO), Nike (NKE), Visa (V) and Procter & Gamble (PG), as well as the S&P 500 (SPY) index.  To begin with, we can observe that the monthly returns of all stocks are nearly normally distributed. Moreover, we can draw two main conclusions:

1. The riskiness of each stock 
2. The probability of distributing a positive monthly return on stockholders

Initially, we can conclude that companies such as Alibaba and Boeing, that show a more flat distribution are the most risky stocks as a large proportion of their monthly returns can be highly positive, but also highly negative. This conclusion is reinforced by examining the table created above, as in fact these two stocks have the distribution with the highest standard deviation. Conversely, the S&P 500 index is the less risky option, with the vast majority of the monthly returns data being concentrated around the mean of the distribution. An interesting finding is that Coca-cola is a rather risk averse stock, an observation that can be attributed potentially to the fact that Coca-cola has managed to sell its product as a commodity, a product that customers are not likely to cut-down irrespectively of their financial status. 

Secondly, by examining the peak monthly returns of each stock, representing the mean of the distributions, we can conclude on which of the stocks can be considered to be more rewarding in the long-run. Observing the data we can see that no company stock has a clearly negative mean of monthly returns. Nevertheless, most stocks have a mean very close to 0, with Alibaba and Visa being the most profitable on the long-run.

We can thus conclude that on the one hand, Alibaba would offer the acute trader the required volatility, as well as a mean positive monthly return to make a profit. On the other hand, S&P500 offers a rather less risky asset with a positive mean of monthly returns as well.


Finally, by making a plot that shows the relationship between the expected monthly return (mean) of a stock and the risk (standard deviation) we can more clearly examine the relative risk vs reward relationship of the stocks.

```{r risk_return_plot}

ggplot(summary_monthly_returns, aes(x = sd, y = mean, color = symbol, label = symbol)) + 
  geom_point() + 
  geom_text_repel() +
  NULL

```

On the above graph we can explore the relationship between the standard deviation with the mean. In fact, the conclusions drawn above are further reinforced through this graph and are even more vivid. Alibaba has indeed a relatively more positive mean of monthly returns than the majority of the other stocks, but is also the most risky of all, an attribute that is expected from the stock of a very fast-growing e-commerce company. Additionally, the Visa stock offers a rather risk averse profile and the highest mean reward. On the contrary, Boeing and AstraZeneca offer a rather low expected return, whilst at the same time being quite risky. 


---
title: "Session 2: Homework 1"
author: "Jiakuo Li"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
```


# On your own: IBM HR Analytics

For this task, you will analyse a data set on Human Resoruce Analytics. The [IBM HR Analytics Employee Attrition & Performance data set](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) is a fictional data set created by IBM data scientists.  Among other things, the data set includes employees' income, their distance from work, their position in the company, their level of education, etc. A full description can be found on the website link provided.


```{r}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
glimpse(hr_dataset)
options(scipen=999)

```

To start, we will be cleaning the data set, as variable names are in capital letters, some variables are not really necessary, and some variables, e.g., `education` are given as a number rather than a more useful description.


```{r}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

```

We will be analysing the data by answering 8 main questions that arise when looking at the data.

1. Q: How often do people leave the company (`attrition`)
   A: 237 out of 1470 people left the company
   
```{r}
hr_cleaned %>% count(attrition=="Yes")
```

2. Q: How are `age`, `years_at_company`, `monthly_income` and `years_since_last_promotion` distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics?
   A: Age distribution is closer to Normal.
   
```{r}
ggplot(hr_cleaned, aes(x=age))+
  geom_histogram(aes(fill=(..count..)))+ #filled with color
  labs(x="Age", y="Frequency", title="2.1 Age Distribution")+
  theme(plot.title = element_text(hjust = 0.5))+ # Centered title+
  scale_fill_continuous(name = "Frequency")+ #adjust legend title
  NULL
ggplot(hr_cleaned,aes(x=years_at_company))+
  geom_histogram(aes(fill=(..count..)))+
  labs(x="Years at Company", y="Frequency", title="2.2 Years at Company Distribution")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_continuous(name = "Frequency")+
  NULL
ggplot(hr_cleaned,aes(x=monthly_income))+
  geom_histogram(aes(fill=(..count..)))+
  labs(x="Monthly Income", y="Frequency", title="2.3 Monthly Income Distribution")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_continuous(name = "Frequency")+
  NULL
ggplot(hr_cleaned,aes(years_since_last_promotion))+
  geom_histogram(aes(fill=(..count..)))+
  labs(x="Years Since Last Promotion", y="Frequency", title="2.4 Years Since Last Promotion Distribution")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_continuous(name = "Frequency")+
  NULL
```

3. Q: How are `job_satisfaction` and `work_life_balance` distributed? Don't just report counts, but express categories as % of total

```{r}
hr_cleaned$job_satisfaction <- factor(hr_cleaned$job_satisfaction,levels = c("Low", "Medium", "High", "Very High"))
hr_cleaned$work_life_balance <- factor(hr_cleaned$work_life_balance,levels = c("Bad", "Good", "Better", "Best"))
#Sorting the X-axis labels by using factor function

ggplot(aes(x = job_satisfaction), data = hr_cleaned)+
geom_bar(aes(y = (..count..)/sum(..count..), fill=..count..))+
  labs(x="Job Satisfaction", y="% of Total", title="3.1 Job Satisfaction Distribution")+
  theme(plot.title=element_text(hjust = 0.5))+
  scale_fill_continuous(name = "% of Total")+
  NULL
ggplot(aes(x = work_life_balance), data = hr_cleaned)+
geom_bar(aes(y = (..count..)/sum(..count..), fill=..count..))+
  labs(x="Work Life Balance", y="% of total", title="3.2 Work Life Balance Distribution")+
  theme(plot.title=element_text(hjust=0.5))+
  scale_fill_continuous(name = "% of Total")+
  NULL
```

4. Q: Is there any relationship between monthly income and education? Monthly income and gender?
   A: Monthly income increases with education level. But there is no significant relationship between male and female in terms of monthly income.
   
```{r}
hr_education <- hr_cleaned %>% 
  group_by(education)
hr_education$education <- factor(hr_education$education,levels = c("Below College", "College", "Bachelor", "Master", "Doctor"))
ggplot(hr_education, aes(x=education, y=monthly_income))+
  geom_boxplot()+
  labs(x="Education", y="Monthly Income", title="4.1 Income and Education Relationship")+
  theme(plot.title=element_text(hjust=0.5))+
  NULL

hr_gender <- hr_cleaned %>% 
  group_by(gender)
ggplot(hr_gender, aes(x=gender, y=monthly_income))+
  geom_boxplot()+
  labs(x="Gender", y="Monthly Income", title="4.2 Income and Gender Relationship")+
  theme(plot.title=element_text(hjust=0.5))+
  NULL
```

5. Q: How is the monthly income dependent on the specific job role within the company? 

```{r}
ggplot(hr_cleaned, aes(x=reorder(job_role, -monthly_income, na.rm = TRUE), y=monthly_income))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90))+
  labs(x="Job Role", y="Monthly Income", title="5. Income and Job Role Relationship")+
  theme(plot.title=element_text(hjust=0.5))+
  NULL
```

6. Q: How is mean income affected by the education level of employees?

```{r}
education_mean <- hr_cleaned %>% 
  group_by(education) %>% 
  summarize(meanIncome=mean(monthly_income))
education_mean$education <- factor(education_mean$education,levels = c("Below College", "College", "Bachelor", "Master", "Doctor"))
ggplot(education_mean, aes(x=education, y=meanIncome, fill=meanIncome))+ #filled with color
  geom_bar(stat="identity")+
  labs(x="Education", y="Mean Income", title="6. Mean Income by Education Level")+
  theme(plot.title=element_text(hjust=0.5))+
  scale_fill_continuous(name = "Mean Income")+ #adjust legend title
  NULL
```

7. How is mean income affected by the education level of employees in the different departments of the firm?

```{r}
ggplot(hr_education, aes(x=education, y=monthly_income))+
  geom_boxplot()+
  facet_wrap(~department)+
  theme_bw()+ 
  theme(axis.text.x = element_text(angle = 90))+
  labs(x="Education", y="Monthly Income", title="7. Income by Education Level(faceted)")+
  theme(plot.title=element_text(hjust=0.5, size=14))+
  NULL
```

8. How is monthly income affected by the age of employees in the different departments of the firm?

```{r}
ggplot(hr_cleaned, aes(x=age, y=monthly_income))+
  geom_point()+
  geom_smooth()+
  facet_wrap(~job_role)+
  labs(x="Age", y="Monthly Income", title="8. Income by Age(faceted)")+
  theme(plot.title=element_text(hjust=0.5))+
  NULL
```

# Challenge 1: Replicating a chart

The aim of this first challenge is to create a publication-ready plot referring the journal article "Riddell_Annals_Hom-Sui-Disparities.pdf". The data we've used to achieve this is- "CDC_Males.csv".This data spans over the 50 states in US, for the years 2008-2016. It contains firearm possession information and causes of death(suicide/homicide) of non-hispanic black and white males. Through the plot, we try to analyse the relation between firearm homicide and suicide among white men along with reported gun ownership.

The original plot from the article can be seen below:

```{r challenge1, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "figure3.jpeg"), error = FALSE)
```

The replicated plot(along with added title) can be seen below:

```{r fig.width=8}

data_set <- vroom(here::here("data", "CDC_Males.csv"))
# glimpse(data_set)

library(ggpubr)
# filter data set to get "Firearm" data
firearm_data <- data_set %>% 
  filter(type=="Firearm")  

# plot suicide rate to homicide rate

replica <- ggplot(firearm_data, aes(x = adjusted.suicide.White, y = adjusted.homicide.White)) +   
  
  # color points by gun ownership and size by population
  geom_point(aes(fill = gun.house.prev.category, size = average.pop.white), col="black", pch=21)+   
  
  # add labels to graph to make it descriptive
  labs(title="Relation between Firearm Homicide and Suicide", subtitle = "For white men, along with reported gun ownership state-wise, years 2008-2016 (R: Spearman's rho)", 
       x="White suicide rate (per 100,000 per year)", y = "White homicide rate (per 100,000 per year)") +  
  
  # using a black and white theme 
  theme_bw()+
  
  # using a red color palette for the scatter points
  scale_fill_brewer(type='seq', palette='Reds')+
  
  # to prevent overlap of labels on the graph
  geom_text_repel(aes(label = ST), size = 3, min.segment.length = 1) + 
  
  # used for scaling points and renaming for legend
  scale_size_area(breaks = c(500000, 1500000, 3000000, 7000000), labels = c("500k", "1.5m", "3m", "7m"), max_size = 20) +     
  
  # renaming legend titles and ordering
  guides(fill = guide_legend(title = "Gun ownership", order=1),size = guide_legend(title = "White population", order=2)) +     

  scale_x_continuous()+
  scale_y_continuous()+
  
  # calculating Spearman's correlation
  stat_cor(method = "spearman", aes(label = ..r.label..))+
  NULL

replica

```



# Challenge 2: 2016 California Contributors plots

As discussed in class, I would like you to reproduce the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.



```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```


To get this plot, you must join two dataframes; the one you have with all contributions, and data that can translate zipcodes to cities.
You can find a file with all US zipcodes, e.g., here http://www.uszipcodelist.com/download.html. 

The easiest way would be to create two plots and then place one next to each other. For this, you will need the `patchwork` package.
https://cran.r-project.org/web/packages/patchwork/index.html

While this is ok, what if one asked you to create the same plot for the top 10 candidates and not just the top two? The most challenging part is how to reorder within categories, and for this you will find Julia Silge's post on [REORDERING AND FACETTING FOR GGPLOT2](https://juliasilge.com/blog/reorder-within/) useful.


```{r, load_CA_data, warnings= FALSE, message=FALSE}
# Make sure you use vroom() as it is significantly faster than read.csv()
CA_contributors_2016 <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))
# install.packages("patchwork")
# install.packages('patchwork')

#Extra Libraries

library(plyr)
library(patchwork)

#Read in data and assign it to a name:

contributors <- vroom("data/CA_contributors_2016.csv")

zip_index <- vroom("data/zip_code_database.csv")

#Merging Data

contributors_full <- merge(contributors,zip_index,by="zip")

# glimpse(contributors_full)

# #Divide Data
# 
# data_clinton <- filter(contributors_full, cand_nm == "Clinton, Hillary Rodham")
#   
# data_trump <- filter(contributors_full, cand_nm == "Trump, Donald J.")
# 
# #Summarize
# 
# data_clinton_summarized <- ddply(data_clinton,"primary_city",numcolwise(sum))
#   
# data_trump_summarized <- ddply(data_trump,"primary_city",numcolwise(sum))

#Sorting
# 
# data_clinton_sorted <- data_clinton_summarized[order(-data_clinton_summarized$contb_receipt_amt),]
#   
# data_trump_sorted <- data_trump_summarized[order(-data_trump_summarized$contb_receipt_amt),]

#Reducing data

# top_10_clinton <- top_n(data_clinton_summarized,10,contb_receipt_amt)
#   
# top_10_trump <- top_n(data_trump_summarized,10,contb_receipt_amt)
# 
# #Plotting
# 
# clinton_plot <- ggplot(top_10_clinton, aes( x = contb_receipt_amt, y = reorder(primary_city,contb_receipt_amt))) +
#   geom_col(fill = "#0015BC", alpha = 8 )+
#   theme_bw()+
#   theme(axis.title.x=element_blank())+
#   theme(axis.title.y=element_blank())+
#   labs(title = "Clinton, Hillary Rodham", x = waiver(), y = waiver())
#   NULL
# 
# trump_plot <- ggplot(top_10_trump, aes( x = contb_receipt_amt, y = reorder(primary_city,contb_receipt_amt))) +
#   geom_col( fill = "#e9141d", alpha = 0.8)+
#   theme_bw()+
#   theme(axis.title.x=element_blank())+
#   theme(axis.title.y=element_blank())+
#   labs(title = "Trump, Donald J.", x = waiver(), y = waiver())
#   NULL
# 
# clinton_plot + trump_plot

```

```{r, load_CA_data, warnings= FALSE, message=FALSE}
 less <- aggregate(contributors_full$contb_receipt_amt, by=list(Candidate=contributors_full$cand_nm), FUN=sum)
 less <- less %>%
   arrange(desc(x))
 top_10 <- head(less,10)
 top_10
```

```{r}
 new_contributors <- contributors_full %>%
   filter(cand_nm %in% top_10$Candidate)
 new_contributors
```
 
```{r}
 library(tidytext)
 new_contributors %>%
     group_by(cand_nm) %>%
     top_n(10) %>%
     ungroup %>%
     mutate(cand_nm = as.factor(cand_nm),
            name = reorder_within(primary_city, contb_receipt_amt, cand_nm)) %>%
     ggplot(aes(primary_city, contb_receipt_amt, fill = cand_nm)) +
     geom_col(show.legend = FALSE) +
     facet_wrap(~cand_nm, ncol=2, scales = "free") +
     coord_flip() +
     scale_x_reordered() +
     scale_y_continuous(expand = c(0,0)) +
     labs(y = "Total contribution",
          x = NULL,
          title = "Top 10 Highest Contributers")
```


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

# Details

- Who did you collaborate with: TYPE NAMES HERE
- Approximately how much time did you spend on this problem set: ANSWER HERE
- What, if anything, gave you the most trouble: ANSWER HERE


**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2021.netlify.app/syllabus/#the-15-minute-rule){target=_blank}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!  

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else? 


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.









